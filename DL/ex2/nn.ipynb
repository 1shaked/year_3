{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "lambda_val = 0.01\n",
    "\n",
    "# split the data into train and test\n",
    "def split_data(data: list, split_ratio : float=0.8):\n",
    "    train_size = int(len(data) * split_ratio)\n",
    "    random.shuffle(data)\n",
    "    return data[:train_size], data[train_size:]\n",
    "# Initialize weights and biases\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer: Z1 = X.W1 + b1, A1 = ReLU(Z1)\n",
    "    linear_l1 = np.dot(X, W1) + b1\n",
    "    relu_res = np.maximum(0, linear_l1)  # ReLU activation\n",
    "\n",
    "    # Output layer: Z2 = A1.W2 + b2, A2 = softmax(Z2)\n",
    "    linear_l2 = np.dot(relu_res, W2) + b2\n",
    "    y = softmax(linear_l2)\n",
    "    \n",
    "    return linear_l1, relu_res, linear_l2, y\n",
    "\n",
    "# Softmax function (numerically stable)\n",
    "def softmax(Z):\n",
    "    # print(\"Z max:\", np.max(Z), \"Z min:\", np.min(Z))\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Add small epsilon for numerical stability\n",
    "    return loss\n",
    "\n",
    "def compute_loss_reg(y_true, y_pred, W1, W2, lambda_val=0.01):\n",
    "    m = y_true.shape[0]\n",
    "    cross_entropy_loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "    reg_loss = (lambda_val / (2 * m)) * (np.sum(W1**2) + np.sum(W2**2))\n",
    "    return cross_entropy_loss + reg_loss\n",
    "\n",
    "# Back propagation\n",
    "def backward_propagation(X, y_true, Z1, A1, Z2, A2, W1, W2):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Output layer gradients\n",
    "    dZ2 = A2 - y_true\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Hidden layer gradients\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * (Z1 > 0)  # Derivative of ReLU\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    dW1 += (lambda_val / m) * W1\n",
    "    dW2 += (lambda_val / m) * W2\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def clip_gradients(gradients, threshold):\n",
    "    for grad in gradients:\n",
    "        np.clip(grad, -threshold, threshold, out=grad)\n",
    "# Gradient descent update\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    clip_gradients([dW1, db1, dW2, db2], threshold=1.0)\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Training loop\n",
    "def train_neural_network(X_train, y_train, hidden_size, learning_rate, epochs, output_size):\n",
    "    input_size = X_train.shape[1]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss_reg(y_train, A2, W1, W2)\n",
    "\n",
    "        # Backward propagation\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W1, W2)\n",
    "\n",
    "        # Update parameters\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    m = y.shape[0]\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    one_hot[np.arange(m), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def train_neural_network_with_batches(X_train_batches, y_train_batches, hidden_size, learning_rate, epochs, output_size):\n",
    "    input_size = X_train_batches.shape[2]\n",
    "    batches_len = X_train_batches.shape[0]\n",
    "    # Initialize parameters\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Initialize total loss for the epoch\n",
    "\n",
    "        for batch in range(batches_len):\n",
    "            X_train = X_train_batches[batch]\n",
    "            y_train = y_train_batches[batch]\n",
    "            # Forward propagation\n",
    "            Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = compute_loss(y_train, A2)\n",
    "            total_loss += loss\n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W1, W2)\n",
    "\n",
    "            # Update parameters\n",
    "            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "            # print(\"Z2 max:\", np.max(Z2), \"Z2 min:\", np.min(Z2))\n",
    "            # print(\"W1 max:\", np.max(W1), \"W1 min:\", np.min(W1))\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "        losses.append(total_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "    return W1, b1, W2, b2, losses\n",
    "\n",
    "def get_x_and_labels(data: list[dict]):\n",
    "    labels = []\n",
    "    X = []\n",
    "    for row in data:\n",
    "        labels.append(row['label'])\n",
    "        del row['label'] \n",
    "        X.append(list(row.values()))\n",
    "    return X, labels\n",
    "\n",
    "def split_to_batches(X, y, batch_size):\n",
    "    X_batches = np.array_split(X, batch_size)\n",
    "    y_batches = np.array_split(y, batch_size)\n",
    "    return X_batches, y_batches\n",
    "\n",
    "def normalize(X):\n",
    "    X = np.array(X)\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>217</td>\n",
       "      <td>220</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      8       0       0       0       0       0       0       0       0   \n",
       "1      4       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       2  ...         0         0         0         6       217       220   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1       141         0         0         0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df.to_dict(orient='records')\n",
    "train_data, test_data = split_data(data)\n",
    "X_train , y_train = get_x_and_labels(train_data)\n",
    "X_test , y_test = get_x_and_labels(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "hidden_size = 128\n",
    "epochs = 1_000\n",
    "output_size = 10  # Number of classes\n",
    "\n",
    "X_train_batches , y_train_batches = split_to_batches(X_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 350, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_batches = np.array(X_train_batches)\n",
    "X_train_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 350), (128, 350, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_batches = np.array(y_train_batches)\n",
    "# Apply one-hot encoding to each batch\n",
    "y_train_batches_one_hot = np.array([one_hot_encode(batch, output_size) for batch in y_train_batches])\n",
    "y_train_batches.shape, y_train_batches_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 132.3870\n",
      "Epoch 10, Loss: 54.3318\n",
      "Epoch 20, Loss: 47.9059\n",
      "Epoch 30, Loss: 44.2393\n",
      "Epoch 40, Loss: 41.5821\n",
      "Epoch 50, Loss: 39.4695\n",
      "Epoch 60, Loss: 37.6768\n",
      "Epoch 70, Loss: 36.1013\n",
      "Epoch 80, Loss: 34.6939\n",
      "Epoch 90, Loss: 33.4151\n",
      "Epoch 100, Loss: 32.2428\n",
      "Epoch 110, Loss: 31.1531\n",
      "Epoch 120, Loss: 30.1317\n",
      "Epoch 130, Loss: 29.1721\n",
      "Epoch 140, Loss: 28.2647\n",
      "Epoch 150, Loss: 27.4044\n",
      "Epoch 160, Loss: 26.5844\n",
      "Epoch 170, Loss: 25.7978\n",
      "Epoch 180, Loss: 25.0420\n",
      "Epoch 190, Loss: 24.3175\n",
      "Epoch 200, Loss: 23.6230\n",
      "Epoch 210, Loss: 22.9532\n",
      "Epoch 220, Loss: 22.3081\n",
      "Epoch 230, Loss: 21.6850\n",
      "Epoch 240, Loss: 21.0832\n",
      "Epoch 250, Loss: 20.5021\n",
      "Epoch 260, Loss: 19.9409\n",
      "Epoch 270, Loss: 19.3970\n",
      "Epoch 280, Loss: 18.8700\n",
      "Epoch 290, Loss: 18.3580\n",
      "Epoch 300, Loss: 17.8608\n",
      "Epoch 310, Loss: 17.3766\n",
      "Epoch 320, Loss: 16.9076\n",
      "Epoch 330, Loss: 16.4517\n",
      "Epoch 340, Loss: 16.0103\n",
      "Epoch 350, Loss: 15.5814\n",
      "Epoch 360, Loss: 15.1673\n",
      "Epoch 370, Loss: 14.7666\n",
      "Epoch 380, Loss: 14.3796\n",
      "Epoch 390, Loss: 14.0033\n",
      "Epoch 400, Loss: 13.6382\n",
      "Epoch 410, Loss: 13.2847\n",
      "Epoch 420, Loss: 12.9408\n",
      "Epoch 430, Loss: 12.6083\n",
      "Epoch 440, Loss: 12.2851\n",
      "Epoch 450, Loss: 11.9720\n",
      "Epoch 460, Loss: 11.6689\n",
      "Epoch 470, Loss: 11.3743\n",
      "Epoch 480, Loss: 11.0889\n",
      "Epoch 490, Loss: 10.8118\n",
      "Epoch 500, Loss: 10.5423\n",
      "Epoch 510, Loss: 10.2797\n",
      "Epoch 520, Loss: 10.0257\n",
      "Epoch 530, Loss: 9.7777\n",
      "Epoch 540, Loss: 9.5377\n",
      "Epoch 550, Loss: 9.3043\n",
      "Epoch 560, Loss: 9.0780\n",
      "Epoch 570, Loss: 8.8574\n",
      "Epoch 580, Loss: 8.6444\n",
      "Epoch 590, Loss: 8.4377\n",
      "Epoch 600, Loss: 8.2367\n",
      "Epoch 610, Loss: 8.0411\n",
      "Epoch 620, Loss: 7.8518\n",
      "Epoch 630, Loss: 7.6665\n",
      "Epoch 640, Loss: 7.4869\n",
      "Epoch 650, Loss: 7.3125\n",
      "Epoch 660, Loss: 7.1432\n",
      "Epoch 670, Loss: 6.9787\n",
      "Epoch 680, Loss: 6.8183\n",
      "Epoch 690, Loss: 6.6638\n",
      "Epoch 700, Loss: 6.5122\n",
      "Epoch 710, Loss: 6.3652\n",
      "Epoch 720, Loss: 6.2226\n",
      "Epoch 730, Loss: 6.0843\n",
      "Epoch 740, Loss: 5.9497\n",
      "Epoch 750, Loss: 5.8179\n",
      "Epoch 760, Loss: 5.6899\n",
      "Epoch 770, Loss: 5.5666\n",
      "Epoch 780, Loss: 5.4463\n",
      "Epoch 790, Loss: 5.3286\n",
      "Epoch 800, Loss: 5.2150\n",
      "Epoch 810, Loss: 5.1031\n",
      "Epoch 820, Loss: 4.9946\n",
      "Epoch 830, Loss: 4.8893\n",
      "Epoch 840, Loss: 4.7865\n",
      "Epoch 850, Loss: 4.6866\n",
      "Epoch 860, Loss: 4.5892\n",
      "Epoch 870, Loss: 4.4948\n",
      "Epoch 880, Loss: 4.4033\n",
      "Epoch 890, Loss: 4.3131\n",
      "Epoch 900, Loss: 4.2260\n",
      "Epoch 910, Loss: 4.1410\n",
      "Epoch 920, Loss: 4.0590\n",
      "Epoch 930, Loss: 3.9785\n",
      "Epoch 940, Loss: 3.9001\n",
      "Epoch 950, Loss: 3.8236\n",
      "Epoch 960, Loss: 3.7502\n",
      "Epoch 970, Loss: 3.6778\n",
      "Epoch 980, Loss: 3.6079\n",
      "Epoch 990, Loss: 3.5393\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, losses = train_neural_network_with_batches(X_train_batches, y_train_batches_one_hot, hidden_size=hidden_size, learning_rate=learning_rate, epochs=epochs, output_size=10)\n",
    "# np.array_split(np.array([0 , 1 , 2 , 3 , 4 , 5]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 345.3107\n",
      "Epoch 10, Loss: 131.7007\n",
      "Epoch 20, Loss: 114.5036\n",
      "Epoch 30, Loss: 103.5163\n",
      "Epoch 40, Loss: 94.5396\n",
      "Epoch 50, Loss: 87.1034\n",
      "Epoch 60, Loss: 80.3993\n",
      "Epoch 70, Loss: 74.5493\n",
      "Epoch 80, Loss: 69.0920\n",
      "Epoch 90, Loss: 64.2377\n",
      "Epoch 100, Loss: 59.7006\n",
      "Epoch 110, Loss: 55.6273\n",
      "Epoch 120, Loss: 51.7581\n",
      "Epoch 130, Loss: 48.2209\n",
      "Epoch 140, Loss: 45.1927\n",
      "Epoch 150, Loss: 42.2669\n",
      "Epoch 160, Loss: 39.3442\n",
      "Epoch 170, Loss: 36.7864\n",
      "Epoch 180, Loss: 34.4232\n",
      "Epoch 190, Loss: 32.2309\n",
      "Epoch 200, Loss: 29.9218\n",
      "Epoch 210, Loss: 28.1000\n",
      "Epoch 220, Loss: 26.3166\n",
      "Epoch 230, Loss: 24.6036\n",
      "Epoch 240, Loss: 23.0711\n",
      "Epoch 250, Loss: 21.7440\n",
      "Epoch 260, Loss: 20.3537\n",
      "Epoch 270, Loss: 19.1570\n",
      "Epoch 280, Loss: 17.9703\n",
      "Epoch 290, Loss: 16.9781\n",
      "Epoch 300, Loss: 15.8866\n",
      "Epoch 310, Loss: 14.9784\n",
      "Epoch 320, Loss: 14.1424\n",
      "Epoch 330, Loss: 13.2702\n",
      "Epoch 340, Loss: 12.5295\n",
      "Epoch 350, Loss: 11.8320\n",
      "Epoch 360, Loss: 11.1343\n",
      "Epoch 370, Loss: 10.5486\n",
      "Epoch 380, Loss: 9.9899\n",
      "Epoch 390, Loss: 9.5360\n",
      "Epoch 400, Loss: 8.9914\n",
      "Epoch 410, Loss: 8.5867\n",
      "Epoch 420, Loss: 8.1251\n",
      "Epoch 430, Loss: 7.7187\n",
      "Epoch 440, Loss: 7.3532\n",
      "Epoch 450, Loss: 7.0104\n",
      "Epoch 460, Loss: 6.6939\n",
      "Epoch 470, Loss: 6.3950\n",
      "Epoch 480, Loss: 6.1152\n",
      "Epoch 490, Loss: 5.8514\n",
      "Epoch 500, Loss: 5.6013\n",
      "Epoch 510, Loss: 5.3567\n",
      "Epoch 520, Loss: 5.1443\n",
      "Epoch 530, Loss: 4.9184\n",
      "Epoch 540, Loss: 4.7316\n",
      "Epoch 550, Loss: 4.5626\n",
      "Epoch 560, Loss: 4.3728\n",
      "Epoch 570, Loss: 4.2049\n",
      "Epoch 580, Loss: 4.0575\n",
      "Epoch 590, Loss: 3.9203\n",
      "Epoch 600, Loss: 3.7779\n",
      "Epoch 610, Loss: 3.6654\n",
      "Epoch 620, Loss: 3.5419\n",
      "Epoch 630, Loss: 3.4139\n",
      "Epoch 640, Loss: 3.3027\n",
      "Epoch 650, Loss: 3.2058\n",
      "Epoch 660, Loss: 3.0990\n",
      "Epoch 670, Loss: 3.0200\n",
      "Epoch 680, Loss: 2.9184\n",
      "Epoch 690, Loss: 2.8491\n",
      "Epoch 700, Loss: 2.7589\n",
      "Epoch 710, Loss: 2.6667\n",
      "Epoch 720, Loss: 2.6136\n",
      "Epoch 730, Loss: 2.5314\n",
      "Epoch 740, Loss: 2.4669\n",
      "Epoch 750, Loss: 2.4082\n",
      "Epoch 760, Loss: 2.3448\n",
      "Epoch 770, Loss: 2.2940\n",
      "Epoch 780, Loss: 2.2224\n",
      "Epoch 790, Loss: 2.1786\n",
      "Epoch 800, Loss: 2.1230\n",
      "Epoch 810, Loss: 2.0778\n",
      "Epoch 820, Loss: 2.0280\n",
      "Epoch 830, Loss: 1.9837\n",
      "Epoch 840, Loss: 1.9389\n",
      "Epoch 850, Loss: 1.8940\n",
      "Epoch 860, Loss: 1.8545\n",
      "Epoch 870, Loss: 1.8195\n",
      "Epoch 880, Loss: 1.7806\n",
      "Epoch 890, Loss: 1.7442\n",
      "Epoch 900, Loss: 1.7092\n",
      "Epoch 910, Loss: 1.6748\n",
      "Epoch 920, Loss: 1.6448\n",
      "Epoch 930, Loss: 1.6104\n",
      "Epoch 940, Loss: 1.5807\n",
      "Epoch 950, Loss: 1.5495\n",
      "Epoch 960, Loss: 1.5259\n",
      "Epoch 970, Loss: 1.4992\n",
      "Epoch 980, Loss: 1.4692\n",
      "Epoch 990, Loss: 1.4451\n",
      "Accuracy: 89.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.19642857142858"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x  # CrossEntropyLoss expects raw logits (no softmax applied here)\n",
    "\n",
    "# Training function\n",
    "def train_nn(model, dataloader, loss_fn, optimizer, epochs):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            # Forward pass\n",
    "            logits = model(batch_X)\n",
    "            loss = loss_fn(logits, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "        # Print epoch loss\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_nn(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            logits = model(batch_X)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Generate PyTorch Dataset and DataLoader\n",
    "def prepare_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)  # Long for CrossEntropyLoss\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Set parameters\n",
    "input_size = 784  # 28x28 images flattened\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming X_train and y_train are your training data (features and labels)\n",
    "# Prepare PyTorch DataLoader\n",
    "train_dataloader = prepare_dataloader(X_train, y_train, batch_size)\n",
    "test_dataloader = prepare_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "# Train the model\n",
    "losses_nn = train_nn(model, train_dataloader, loss_fn, optimizer, epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_nn(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe7db129d50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(losses, label='Custom NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
